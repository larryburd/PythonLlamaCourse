{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Llama\n",
    "\n",
    "Now that we have some knowledge of the fundamentals of how llama 2 is representing data, both within its model\n",
    "architecture itself as well as the sequences of tokens coming in and out of the model, we can start to query the model.\n",
    "Now, I've used the word \"query\" here, but I don't mean it in the relational database sense, but in the human language\n",
    "sense: to ask a question. A database query is deterministic -- if the data hasn't changed and the query hasn't changed\n",
    "then the same result is always returned. But a language model is a bit more nondeterministic, a more precise word to\n",
    "describe our interaction with the model is \"inference\", or \"predict\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write our first llama.cpp based application.\n",
    "# To create a new model to query we have to identify the quantized\n",
    "# file which we want to use. In this course we have two models created\n",
    "# for experimentation, one being the base 13B paramneter model and the\n",
    "# other being the chat-tuned 13B parameter model. We'll explore the\n",
    "# base model first.\n",
    "\n",
    "# Read in the path for the model file\n",
    "import os\n",
    "\n",
    "model_path: str = os.environ[\"LLAMA_13B\"]\n",
    "\n",
    "# Import the llama.cpp python bindings and load the model\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model: Llama = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all we've done is load the model and llama.cpp has given us a ton of debug information! This is worth taking a quick\n",
    "look at, especially if you might end up using multiple kinds of models, either different architectures or different\n",
    "quantization levels.\n",
    "\n",
    "We see that this is a llama 2 model, that the context length is up to 4096 tokens - and we'll talk about that in a\n",
    "moment - some information about the special tokens which exist for the moment, and if we scroll down we can see the\n",
    "number of layers in the model, which speaks to the internal data structure, and whether these have been configured to be\n",
    "offloaded onto a GPU or not. One of the interesting options in llama.cpp is that you don't have to choose between the\n",
    "CPU or the GPU, you can do a bit of both depending upon your hardware setup.\n",
    "\n",
    "As we get to the bottom here we see the `llama_new_context_with_model` log lines, and this indicates the default seems\n",
    "to be a context of 521 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we go further I want to reduce the verbosity of the output\n",
    "# a bit -- feel free to leave this set to True to see more of the\n",
    "# nitty gritty details of what's happening!\n",
    "model.verbose = False\n",
    "\n",
    "# Now that we have a model let's actually do some inference! The\n",
    "# method we use for this is called create_completion(), and by\n",
    "# default we only need to pass it a prompt to complete and it\n",
    "# returns to us a Completion object, which is a TypedDict.\n",
    "\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "result: Completion = model.create_completion(prompt=\"The capital of Michigan is \")\n",
    "\n",
    "# The Completion type has a choices key which shows us the list of\n",
    "# responses the LLM generated, let's take a look\n",
    "print(result[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So, if you are following along in the notebook you undoubtedly have a different response than I'm showing here.\n",
    "That's what we mean by inference, the model is making a series of probabilistic choices based on the input sequence and\n",
    "the weights to generate this output sequence. We can make the model behave more deterministically by setting it's\n",
    "`temperature`, which is a parameter from zero to one where values closer to zero cause the model will behave more\n",
    "deterministically and values closer to one cause the model to behave more non-deterministic, and creative. Let's do a\n",
    "little experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a few different temperature values\n",
    "temps: list[float] = [0.0, 0.5, 1.0]\n",
    "\n",
    "# Now, for each of these temperatures, let's do three completions\n",
    "prompt: str = \"The planets in the solar system include \"\n",
    "for temp in temps:\n",
    "    for i in range(0, 3):\n",
    "        result: Completion = model.create_completion(prompt=prompt, temperature=temp)\n",
    "        print(f'temp={temp}, run={i}, result: {result[\"choices\"][0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can see that the low temperature returns really consistent results, and that the high remperature gives\n",
    "different kinds of answers. Now, don't go reading into the tea leaves too much here, but play with this, experiment and\n",
    "get a feeling for how the temperature effects your prompt completions. Underneath the temperature is changing how the\n",
    "next token is picked from a set of candidate tokens, so higher levels of temperature will deviate more quickly from one\n",
    "another on repeated querying.\n",
    "\n",
    "You've undoubtedly noticed that we're just getting short little responses here. Generating tokens is slow, and by\n",
    "default the `chat_completion()` method limits the number of tokens returned to just 16. We can change this to -1 to\n",
    "generate as many tokens as available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just do one run, and we'll leave the temperature at\n",
    "# it's default value, which is 0.8\n",
    "result: Completion = model.create_completion(prompt=prompt, max_tokens=-1)\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the first thing you'll notice, especially if you are following along in the lab workspaces with me, is that it's\n",
    "slow. We're already running this model in a quantized form, so to speed things up further we need to either get better\n",
    "hardware, or further reduce the model size, perhaps through additional quantization. You'll also likely notice -- and I\n",
    "say likely because everything here is non-deterministic -- that the model still doesn't just \"finish\". It trails off\n",
    "eventually, but not at 16 characters. And this comes down to the content length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Length\n",
    "\n",
    "Once you understand tokenization the context length is really simple, it's just the maximum length of the token sequence\n",
    "that the model is trained on. In the case of the original llama model the context length was just over two thousand\n",
    "tokens (2,024), which means the input data used for training the model was broken up into a maximum this length of\n",
    "sequence. For llama 2 that was increased to over four thousand tokens (4,096).\n",
    "\n",
    "You can think of the context length like the amount of \"memory\" that an LLM has for a given query. The bigger it is the\n",
    "more you can put in the query and thus the more the LLM will be aware of when return to you a response. Training -- and\n",
    "inference -- with a large context length can increase quality of the output, but they do so at the cost of increased\n",
    "computation. When you create a new `Llama()` object in llama.cpp the default maximum context length is set to 512\n",
    "tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do one last demo, and here I want to show you how we don't have to\n",
    "# wait for whole query to finish, but can instead use the streaming features\n",
    "# of llama.cpp to see tokens as they are completed.\n",
    "\n",
    "# I'm going to create a new model with a nice large context size\n",
    "model: Llama = Llama(model_path=model_path, verbose=False, n_ctx=4096)\n",
    "\n",
    "# If we pass the stream=True parameter to create_completion() we will get back\n",
    "# an iterator of CreatCompletionStreamResponse objects, which are just typed\n",
    "# dictionaries similar to the Completion type\n",
    "token_count: int = 0\n",
    "for result in model.create_completion(\n",
    "    prompt=\"Some fun things to do for vacation in the state of Michigan includes \",\n",
    "    max_tokens=-1,\n",
    "    stream=True,\n",
    "):\n",
    "    # I'm only going to print a newline every 50 tokens or so\n",
    "    if token_count % 50 == 0:\n",
    "        print(\"\")\n",
    "    token_count = token_count + 1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, while you learn a little bit more about all the wonderful things you can do on vacation here in the State of\n",
    "Michigan, I'm going to go grab a diet coke and refresh my voice -- I'll see you in the next video!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
