{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterizing the Llama 2 Model\n",
    "\n",
    "We've talked about some of the fundamentals of large language models -- llama 2 specifically -- and how they are\n",
    "nondeterministic probabalistic machines which predict token sequences based on their pretrained weights. If we want to\n",
    "(and I don't think this is done in deployment very often) we can actually observe some of the inner workings of this\n",
    "choice in llama.cpp by configuring the model when we load it. This is going to be helpful in getting a better\n",
    "understanding of how we can change the behavior of the model, so let's take a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llama.cpp python bindings and load the model, this time we are\n",
    "# going to indicate to the model that we want to capture the probabilities\n",
    "# of tokens for analysis as well, we do this by setting logits_all=True.\n",
    "# I'll also include the llama types just for code cleanliness and completeness\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "model: Llama = Llama(\n",
    "    model_path=os.environ[\"LLAMA_13B\"], verbose=False, logits_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we send a prompt to the model we can pass in the logprobs parameter which\n",
    "# will determine how many alternative choices we will see. I'll just look at the\n",
    "# top two choices. And we'll set the temperature to 0, which means that we'll\n",
    "# have the model always choose from that top 3 the very best choice\n",
    "result: Completion = model.create_completion(\n",
    "    prompt=\"The capital of Michigan is \", logprobs=3, max_tokens=16, temperature=0\n",
    ")\n",
    "\n",
    "# Take out the response content from the Completion object\n",
    "item = result[\"choices\"][0]\n",
    "\n",
    "# Print out the response text\n",
    "print(item[\"text\"])\n",
    "\n",
    "# The individual token probabilities are in the CompletionLogprobs object\n",
    "details: CompletionLogprobs = item[\"logprobs\"]\n",
    "\n",
    "# Here we can look at the list of tokens, their probabilities, and their offset\n",
    "# in the prompt+response\n",
    "print(details[\"tokens\"])  # these will actually be encoded (text) copies of tokens!\n",
    "print(\n",
    "    details[\"token_logprobs\"]\n",
    ")  # the probability of the token in the set of candidates\n",
    "print(details[\"text_offset\"])  # the location in the prompt+response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is interesting, even if it's not something you're likely to do in production very often. It lets you see not\n",
    "only what the tokens were, but how likely they were to fit into the response. Underneath, the goal of the model is to\n",
    "choose the magnitude of the token which is closest to zero as often as possible, but to do so while respecting the\n",
    "temperature parameter. Now, that's a little bit of an oversimplification, actually the model considers another parameter\n",
    "called top_p as well. This parameter determines how many words are in the candidate set for us to consider. temperature\n",
    "and top_p work differently, but together, to determine which token is chosen. For both parameters a higher number gives\n",
    "more options, and thus less deterministic output.\n",
    "\n",
    "Let's dig in just a bit more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can actually see for each token what the alternatives were and what their\n",
    "# logprob was. We can iterate through the set of tokens and, since I set logprobs=3,\n",
    "# we will see the top three options for each token\n",
    "for token_choice in details[\"top_logprobs\"]:\n",
    "    print(token_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Now, there are several parameters we can use to constrain or guide what the language model might choose for the next\n",
    "token. Going into them in detail is outside of the scope of this short course, but I want to give you a bit of an\n",
    "intuitive understanding of how they work. The first one, which we've already talked a bit about, is the `temperature`,\n",
    "and we can use our newfound ability to find probabilities of tokens to see better how this is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the temperature was set to 0 the very first token, the one which\n",
    "# minimized the absolute value of the logprob, was chosen. We can experiment\n",
    "# a bit with this -- let's turn up the heat and see what happens\n",
    "result: Completion = model.create_completion(\n",
    "    prompt=\"The capital of Michigan is \", logprobs=3, max_tokens=16, temperature=0.7\n",
    ")\n",
    "\n",
    "details: CompletionLogprobs = result[\"choices\"][0][\"logprobs\"]\n",
    "\n",
    "# Here we can look at the list of tokens, their probabilities, and their offset\n",
    "# in the prompt+response\n",
    "print(details[\"tokens\"])\n",
    "print()  # little whitespace to see better\n",
    "\n",
    "# iterate through results\n",
    "for token_choice in details[\"top_logprobs\"]:\n",
    "    print(token_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So here we can see that the sequence returned isn't made up of each of the lowest tokens in the candidate sets. In\n",
    "fact, depending on a little bit of randomness, you might even see that several of the candidate sets have more than the\n",
    "3 tokens in them -- that the high temperature resulted in the model choosing a token that wasn't in the top three\n",
    "options. This important to note too -- even though we set the logprobs to three, that's not the complete candidate set\n",
    "of tokens, but just the ones which are being returned to us programmatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `top_k`\n",
    "\n",
    "Another parameter we have in our toolbox is called `top_k`, and this more forcefully constraints the language model,\n",
    "telling it _only_ to choose from a fixed set of best possible matches. This parameter is an integer value, and if we set\n",
    "it to one, for instance, the model will have only one choice to choose from and the temperature won't matter. Let's take\n",
    "a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now constrain the set of choices the model can consider by setting top_k\n",
    "# to two. In this world the model only has a choice between the two best fitting\n",
    "# tokens for a position in the output sequence.\n",
    "result: Completion = model.create_completion(\n",
    "    prompt=\"The capital of Michigan is \",\n",
    "    logprobs=3,\n",
    "    max_tokens=16,\n",
    "    temperature=1.0,\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "details: CompletionLogprobs = result[\"choices\"][0][\"logprobs\"]\n",
    "\n",
    "# Here we can look at the list of tokens, their probabilities, and their offset\n",
    "# in the prompt+response\n",
    "print(details[\"tokens\"])\n",
    "print()  # little whitespace to see better\n",
    "\n",
    "# iterate through results\n",
    "for token_choice in details[\"top_logprobs\"]:\n",
    "    print(token_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see here that because logprobs is set to three I'm still being shown potential tokens, but they are pruned from the\n",
    "candidate set before the model samples from the list as the `top_k` value is only two. By default the `top_k` value is\n",
    "40, and you can see how it works with the temperature to determine which tokens are placed in the output sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `top_p`\n",
    "\n",
    "We also have another variable we can use for tuning, called `top_p`. This variable is a bit more complex, it constrains\n",
    "the candidate set of tokens based upon their cummulative probability distribution. It's a floating point number from 0\n",
    "through 1, where 1 would be all possible tokens from our vocabulary. You can think of this as follows:\n",
    "\n",
    "1. Each candidate token has a probability of being chosen and put in the output sequence.\n",
    "2. We want to only choose from a set of really good options, so we take a number of high probability items and we're\n",
    "   going to put them in the candidate set.\n",
    "3. We don't want to constrain the set to only ten or fifteen items. Instead, once the number of items in that set have a\n",
    "   combined probability equal to `top_p`, then we will stop adding items to the set.\n",
    "\n",
    "So, it's a lot like `top_k` as the goal is to constrain the size of the candidate set, but for each position in the\n",
    "output sequence the number of items we choose from will vary depending on how high of a probability the items have.\n",
    "Let's take a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now constrain the set of choices the model can consider by setting top_p\n",
    "# to a very low number. By default it's 0.95, but if we constrain it to 0.01\n",
    "# we should be able to get a small set of high quality options.\n",
    "result: Completion = model.create_completion(\n",
    "    prompt=\"The capital of Michigan is \",\n",
    "    logprobs=3,\n",
    "    max_tokens=16,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    ")\n",
    "\n",
    "details: CompletionLogprobs = result[\"choices\"][0][\"logprobs\"]\n",
    "\n",
    "# Here we can look at the list of tokens, their probabilities, and their offset\n",
    "# in the prompt+response\n",
    "print(details[\"tokens\"])\n",
    "print()  # little whitespace to see better\n",
    "\n",
    "# iterate through results\n",
    "for token_choice in details[\"top_logprobs\"]:\n",
    "    print(token_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of such a small `top_p` is that the candidate set of choices is very small, though not all of the choices are\n",
    "good ones. Since this is a smaller set of choices, and the probability of any given choice in the set can vary wildly,\n",
    "depending on the pretraining of the model. However, the tokens in the candidate set are reweighted as they are the only\n",
    "options, and the temperature then determines how likely an given candidate will be chosen.\n",
    "\n",
    "Now, all of this is a bit of an oversimplification, and there are several more parameters some of which you can tune,\n",
    "such as the `typical_p`, `min_p`, and so forth. If you're interested more in how these generative models choose a given\n",
    "response, from a statistics and deep learning lens, I'd encourage you to check out paper by Holtzman et al. that I've\n",
    "put in the optional readings for this week, or read more on topics such as nucleus sampling, top_k sampling, and beam\n",
    "search.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
