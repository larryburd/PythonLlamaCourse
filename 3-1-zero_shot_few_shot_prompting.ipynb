{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero and Few Shot Prompting\n",
    "\n",
    "Writing good prompts for large language models is a combination of art, science and, perhaps a bit of wizardry. As you\n",
    "know, the model is a statistical one, taking numeric input sequences and creating an output sequence to match based on\n",
    "the (a) pretrained data, (b) the input sequence itself, and (c) the various parameters we can use when calling the\n",
    "model. In this lecture I want to introduce you to three different prompting strategies. I caution that there are many\n",
    "more strategies out there, but these two form a nice foundation for current practices and they work well with llama 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Prompting\n",
    "\n",
    "You've actually seen zero shot prompting many times in this course -- it's simply giving the model a single set of\n",
    "instructions to respond to, and letting the model use it's pretrained weights. And actually, calling the inputs as\n",
    "\"instructions\" is questionable at best -- many of us are use to the chat gpt conversational style, but the model is\n",
    "actually just doing output sequence prediction. So zero shot prompting is giving an input sequence and taking from the\n",
    "model a predicted appropriate output sequence.\n",
    "\n",
    "Throughout this lecture I'm going to use a really authentic task for myself -- you see, I teach a lot of programming in\n",
    "my day to day job at the University of Michigan and here on the Coursera platform. This often coveres topics in the\n",
    "areas of python, data science, and applied AI. One of the things I'd like to do is give students more quick questions to\n",
    "test their knowledge. But, coming up with good questions is difficult, and then I have to type them all out and enter\n",
    "them into a quizzing tool to be delivered to the learner. Most of these tools can take JSON formatted questions, but I\n",
    "find typing out JSON documents as slow and error prone. So let's see if we can build a conversational agent to help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup our lamma model, and this time I'm going to bump up the context\n",
    "# window a bit. This can slow things down, but also will result in more output\n",
    "# tokens being sent back to us.\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "model: Llama = Llama(\n",
    "    model_path=os.environ[\"LLAMA_13B\"], verbose=False, n_ctx=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, in zero shot prompting we're just asking the language model to\n",
    "# continue our text it's pretrained weights. Since I want to generate\n",
    "# some python 3 lambda questions in JSON, this seems like a good\n",
    "# starting point!\n",
    "\n",
    "prompt = \"Python 3 lambda question in JSON:\"\n",
    "\n",
    "# Now let's watch the results. Remember you need to increase the\n",
    "# max_tokens as well as the context window or llama.cpp will cut\n",
    "# off the reply\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, well, that doesn't seem like a completely unreasonable response to the prompt, but it's certainly not what I was\n",
    "looking for. Let's try another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a little tweak, trying to write the prompt as if it were\n",
    "# something that was observed in the training data, e.g. a textbook\n",
    "prompt = \"A good Python 3 lambda question rendered in JSON is \"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting\n",
    "\n",
    "Large language models are statistical pattern matching machines, and the idea behind few-shot prompting is that we can\n",
    "give examples in our prompt to help the model tailor its output to what we are looking for. This turns out to be a sort\n",
    "of very simple super power for prompt engineering, and is very helpful when you want to constrain the responses from a\n",
    "model to a specific format. Let's see if it helps us here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm increasing the size of my prompt, but I'm going back to the format I had\n",
    "# previously. I've intentionally done two things here: (a) a macro pattern, where\n",
    "# I indicate I'm looking for a question in JSON and I just vary the topic, and\n",
    "# (b) a format pattern, where I show what I want the output to look like.\n",
    "\n",
    "prompt = \"\"\"Python 3 lambda question in JSON:\n",
    "{\"question\":\"The lambda keyword in python is:\",\"correct_answer\":\"For declaring anonymous functions\",\"incorrect_answer\":\"For mathematical operations\"}\n",
    "\n",
    "Python 3 def question in JSON:\n",
    "{\"question\":\"What does the 'def' keyword do?\",\"correct_answer\":\"Define a function\",\"incorrect_answer\":\"Declare variables\"}\n",
    "\n",
    "Python 3 assert question in JSON: \n",
    "\"\"\"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Instantly we get basically what I was looking for -- a question on the topic of asserts in python 3. Sometimes when\n",
    "I run this I also get a number of other questions, with the model just continuing the pattern and going through a list\n",
    "of python keywords and topics and giving me output. I don't always want this, as I don't cover all of the topics it\n",
    "might generate text for. Let's tweak this a bit more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\":\"What is an assertion?\",\"correct_answer\":\"A statement that evaluates to True or False.\",\"incorrect_answer\":\"A statement that is evaluated with the 'assert' keyword.\"}\n",
      "\n",
      "Python 3 with question in JSON: \n",
      "{\"question\":\"The 'with' keyword can be used for which of the following?\",\"correct_answer\":\"Opening and closing a file\",\"incorrect_answer\":[\"Closing a file\",\"Opening and closing a file\"]}\n",
      "\n",
      "Python 3 import question in JSON: \n",
      "{\"question\":\""
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Now I'm just adding the topics at the very beginning. I expect that the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# is going to recognize the patterns here, seeing the list of topics, are repeated\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# in the individual prompts, and that it will follow and just give me results\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# for the python 3 assert, with, and import keywords.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mTopics: lambda, def, assert, with, import.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mPython 3 lambda question in JSON:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mPython 3 assert question in JSON: \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m response \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mcreate_completion(prompt, max_tokens\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     result \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1046\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1044\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1047\u001b[0m     prompt_tokens,\n\u001b[1;32m   1048\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1049\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m   1050\u001b[0m     min_p\u001b[39m=\u001b[39mmin_p,\n\u001b[1;32m   1051\u001b[0m     typical_p\u001b[39m=\u001b[39mtypical_p,\n\u001b[1;32m   1052\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m   1053\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m   1054\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1055\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1056\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1057\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1058\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1059\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1060\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1061\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1062\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m   1063\u001b[0m ):\n\u001b[1;32m   1064\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m llama_cpp\u001b[39m.\u001b[39mllama_token_is_eog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel, token):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:709\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39m# Eval and sample\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    710\u001b[0m     \u001b[39mwhile\u001b[39;00m sample_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens:\n\u001b[1;32m    711\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    712\u001b[0m             top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    713\u001b[0m             top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m             idx\u001b[39m=\u001b[39msample_idx,\n\u001b[1;32m    728\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:547\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    543\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch\u001b[39m.\u001b[39mset_batch(\n\u001b[1;32m    545\u001b[0m     batch\u001b[39m=\u001b[39mbatch, n_past\u001b[39m=\u001b[39mn_past, logits_all\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_params\u001b[39m.\u001b[39mlogits_all\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch)\n\u001b[1;32m    548\u001b[0m \u001b[39m# Save tokens\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[n_past : n_past \u001b[39m+\u001b[39m n_tokens] \u001b[39m=\u001b[39m batch\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py:315\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39massert\u001b[39;00m batch\u001b[39m.\u001b[39mbatch \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_decode(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    317\u001b[0m     batch\u001b[39m.\u001b[39;49mbatch,\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_decode returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now I'm just adding the topics at the very beginning. I expect that the model\n",
    "# is going to recognize the patterns here, seeing the list of topics, are repeated\n",
    "# in the individual prompts, and that it will follow and just give me results\n",
    "# for the python 3 assert, with, and import keywords.\n",
    "prompt = \"\"\"Topics: lambda, def, assert, with, import.\n",
    "\n",
    "Python 3 lambda question in JSON:\n",
    "{\"question\":\"The lambda keyword in python is:\",\"correct_answer\":\"For declaring anonymous functions\",\"incorrect_answer\":\"For mathematical operations\"}\n",
    "\n",
    "Python 3 def question in JSON:\n",
    "{\"question\":\"What does the 'def' keyword do?\",\"correct_answer\":\"Define a function\",\"incorrect_answer\":\"Declare variables\"}\n",
    "\n",
    "Python 3 assert question in JSON: \n",
    "\"\"\"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, things are getting exciting and I've just about automated my weekend job! I actually just want the JSON\n",
    "results, and sometimes (though maybe not in the case in your notebook if you are following along!) the result doesn't\n",
    "have all of the JSON fields I might want. Remember, everything is tokens and sequences, and the input prompt is a\n",
    "statistical machine, so there are a couple of things we might tweak further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"question\":\"The assert statement is used for:\",\n",
      "\"correct_answer\":\"For asserting or verifying conditions in tests\",\n",
      "\"incorrect_answer\":\"To make variable assignment statements more concise\"\n",
      "},\n",
      "{\n",
      "\"question\":\"What does the 'with' statement do?\",\n",
      "\"correct_answer\":\"Defines a block of code that is guaranteed to be executed once\",\n",
      "\"incorrect_answer\":\"Creates an object for each iteration in a loop\"\n",
      "},\n",
      "{\n",
      "\"question\":\"The import keyword in python can be used to:\",\n",
      "\"correct_answer=\"Import a module or package\",\n",
      "\"in"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# I'm going to include the list of topics at the top, then I'm going to\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# use some whitespace formatting on the JSON with newlines to see if this\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# helps increase adherence to the format while keeping the semantics.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# everything except the first line of my\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpython_3_topics\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m = [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlambda\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdef\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39massert\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimport\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m],questions=[\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m{\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m{\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m response \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mcreate_completion(prompt, max_tokens\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     result \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/3-1-zero_shot_few_shot_prompting.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1046\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1044\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1047\u001b[0m     prompt_tokens,\n\u001b[1;32m   1048\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1049\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m   1050\u001b[0m     min_p\u001b[39m=\u001b[39mmin_p,\n\u001b[1;32m   1051\u001b[0m     typical_p\u001b[39m=\u001b[39mtypical_p,\n\u001b[1;32m   1052\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m   1053\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m   1054\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1055\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1056\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1057\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1058\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1059\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1060\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1061\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1062\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m   1063\u001b[0m ):\n\u001b[1;32m   1064\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m llama_cpp\u001b[39m.\u001b[39mllama_token_is_eog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel, token):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:709\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39m# Eval and sample\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    710\u001b[0m     \u001b[39mwhile\u001b[39;00m sample_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens:\n\u001b[1;32m    711\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    712\u001b[0m             top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    713\u001b[0m             top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m             idx\u001b[39m=\u001b[39msample_idx,\n\u001b[1;32m    728\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:547\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    543\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch\u001b[39m.\u001b[39mset_batch(\n\u001b[1;32m    545\u001b[0m     batch\u001b[39m=\u001b[39mbatch, n_past\u001b[39m=\u001b[39mn_past, logits_all\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_params\u001b[39m.\u001b[39mlogits_all\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch)\n\u001b[1;32m    548\u001b[0m \u001b[39m# Save tokens\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[n_past : n_past \u001b[39m+\u001b[39m n_tokens] \u001b[39m=\u001b[39m batch\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py:315\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39massert\u001b[39;00m batch\u001b[39m.\u001b[39mbatch \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_decode(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    317\u001b[0m     batch\u001b[39m.\u001b[39;49mbatch,\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_decode returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# I'm going to include the list of topics at the top, then I'm going to\n",
    "# use some whitespace formatting on the JSON with newlines to see if this\n",
    "# helps increase adherence to the format while keeping the semantics.\n",
    "# everything except the first line of my\n",
    "\n",
    "prompt = \"\"\"\n",
    "{\"python_3_topics\" = [\"lambda\", \"def\", \"assert\", \"with\", \"import\"],questions=[\n",
    "{\n",
    "\"question\":\"The lambda keyword in python is:\",\n",
    "\"correct_answer\":\"For declaring anonymous functions\",\n",
    "\"incorrect_answer\":\"For mathematical operations\"\n",
    "},\n",
    "{\n",
    "\"question\":\"What does the 'def' keyword do?\",\n",
    "\"correct_answer\":\"Define a function\",\n",
    "\"incorrect_answer\":\"Declare variables\"\n",
    "},\n",
    "{\n",
    "\"\"\"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I think that's pretty solid. What's really cool about this, I think, is that I'm not conversing with the model, or\n",
    "trying to prime it to be an expert. I just started a JSON document and it captured both the meaning of what I was doing\n",
    "-- writing questions with correct and incorrect answers -- and the syntax of what I was doing -- writing well formed\n",
    "JSON. All this on a quantized 13B parameter model!\n",
    "\n",
    "This is a great time to jump into the notebooks and experiment a bit yourself to see this in action. Here are a couple\n",
    "of nice tasks for you to try and practice what you've learned; first, how would you reimplement this code using the\n",
    "llama 2 chat model?, and second, how would you rewrite the prompts so that there were multiple incorrect answers, all in\n",
    "a JSON list of their own? Give these a shot in the labs workspace.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
