{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "model: Llama = Llama(\n",
    "    model_path=os.environ[\"LLAMA_13B\"], verbose=False, n_ctx=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\begin{code}\n",
      "import json\n",
      "\n",
      "# Load the input file, and store it as a dictionary.\n",
      "my_data = json.load(open('test.json', 'r'))\n",
      "\n",
      "# For each key of the dictionary:\n",
      "for k in my_data.keys():\n",
      "    # Look at its value, which is a list:\n",
      "    for v in my_data[k]:\n",
      "        # If it's an object, and if it has a \"name\" field,\n",
      "        # then we'll make our own little lambda function from the name field\n",
      "        if type(v) == dict and 'name' in v.keys():\n",
      "            # Now I can do whatever I want with this lambda\n",
      "            print('The name is ' + v['name'] + ', here it is:')\n",
      "\\end{code}\n",
      "\n",
      "I get the error:\n",
      "\n",
      "\\begin{blockquote}\n",
      "TypeError: 'NoneType' object is not callable\n",
      "\\end{blockquote}\n",
      "\n",
      "My JSON file:\n",
      "\n",
      "\\begin{code}\n",
      "{\"user_id\":\"1\",\"name\":\"John Doe\",\"email\":\"john.doe@gmail.com\",\"gender\":\"male\",\"age\":37,\"education\":[],\"interests\":[],\"location\":\"New York\"}\n",
      "{\"user_id\":\""
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/project/labs/Module_3-1.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/Module_3-1.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPython 3 lambda question in JSON:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/Module_3-1.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m response \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mcreate_completion(prompt, max_tokens\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/Module_3-1.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     result \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://syyukkemyldb.labs.coursera.org/home/coder/project/labs/Module_3-1.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1046\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1044\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1047\u001b[0m     prompt_tokens,\n\u001b[1;32m   1048\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1049\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m   1050\u001b[0m     min_p\u001b[39m=\u001b[39mmin_p,\n\u001b[1;32m   1051\u001b[0m     typical_p\u001b[39m=\u001b[39mtypical_p,\n\u001b[1;32m   1052\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m   1053\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m   1054\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1055\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1056\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1057\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1058\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1059\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1060\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1061\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1062\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m   1063\u001b[0m ):\n\u001b[1;32m   1064\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m llama_cpp\u001b[39m.\u001b[39mllama_token_is_eog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel, token):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:709\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39m# Eval and sample\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    710\u001b[0m     \u001b[39mwhile\u001b[39;00m sample_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens:\n\u001b[1;32m    711\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    712\u001b[0m             top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    713\u001b[0m             top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m             idx\u001b[39m=\u001b[39msample_idx,\n\u001b[1;32m    728\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:547\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    543\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch\u001b[39m.\u001b[39mset_batch(\n\u001b[1;32m    545\u001b[0m     batch\u001b[39m=\u001b[39mbatch, n_past\u001b[39m=\u001b[39mn_past, logits_all\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_params\u001b[39m.\u001b[39mlogits_all\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch)\n\u001b[1;32m    548\u001b[0m \u001b[39m# Save tokens\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[n_past : n_past \u001b[39m+\u001b[39m n_tokens] \u001b[39m=\u001b[39m batch\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py:315\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39massert\u001b[39;00m batch\u001b[39m.\u001b[39mbatch \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_decode(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    317\u001b[0m     batch\u001b[39m.\u001b[39;49mbatch,\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_decode returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"A good Python 3 lambda question in JSON is \"\n",
    "\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 characters long.\n",
      "The \"Python 3 Lambda Question In Json\" question was asked on January 5th, 2019 by a member playing the game Who Wants To Be A Millionaire?. The first answer is usually apparent from the question, and in the case at hand it would therefore be:\n",
      "A lambda expression with a JSON output.\n",
      "A Python 3 lambda question in JSON is 800 characters long. - Who Wants To Be A Millionaire? (January 5th, 2019)"
     ]
    }
   ],
   "source": [
    "prompt = \"A good Python 3 lambda question in JSON is \"\n",
    "\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\":\"What does the assert keyword do?\",\"correct_answer\":\"Make sure that an expression is true\",\"incorrect_answers\":[\"It asserts a statement\"]}\n",
      "\n",
      "Python 3 list comprehension question in JSON:\n",
      "{\"question\":\"What does [x for x in y] create?\",\"correct_answer\":\"A list of all items from list y\",\"incorrect_answer\":\"An item from list y\"}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python 3 lambda question in JSON:\n",
    "{\"question\":\"The lambda keyword in python is:\",\"correct_answer\":\"For declaring anonymous functions\",\"incorrect_answer\":\"For mathmatics\"}\n",
    "\n",
    "Python 3 def question in JSON:\n",
    "{\"quesion\":\"What does the 'def' keyword do?\",\"correct_answer\":\"Define a function\",\"incorrect_answer\":\"Declare variables\"}\n",
    "\n",
    "Python 3 assert question in JSON:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
